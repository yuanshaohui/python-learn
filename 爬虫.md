## 目录
[toc]
## requests对象的使用
1. User-Agent动态池的实现
    - 导入`form randon import chioce`,用`chioce(列表)`方式。
    - 特殊库；`from fake_useragent import UserAgent`,调用对象`ua = UserAgent,ua.chrome
```python
from urllib.request import Request
from urllib.request import urlopen
from random import choice


url = 'https://maoyan.com/board'
# 伪装头池
agents = ['Mozilla/5.0(Macintosh;U;IntelMacOSX10_6_8;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50','Mozilla/5.0(Macintosh;IntelMacOSX10.6;rv:2.0.1)Gecko/20100101Firefox/4.0.1']
headers = {
    'User-Agent': choice(agents)
} 
request = Request(url, headers = headers)
response = urlopen(request)
info = response.read()
print(info.decode())
```
## get的使用
1. 'quote':当get网址中有中文，需要串码时.`from urllib.parse import quote`调用`quote("转码中文")`
2. 'urlencode':有多个参数转码时，`from urllib.parse import urlencode`,调用`args{"wd"="转码内容"}，print(urlencode(arges)`
## 贴吧案例
 ```python
 from urllib.request import Request, urlopen
from urllib.parse import urlencode
from fake_useragent import UserAgent


def get_html(url):
    headers = {
        "User-Agent": UserAgent().chrome
    }
    request = Request(url, headers=headers)
    response = urlopen(request)
    # print(response.read().decode())
    return response.read()


def save_html(file_name, html_byte):  # 二进制，不用encode。
    with open(file_name, "wb") as f:
        f.write(html_byte)


def main():
    content = input("请输入贴吧名字：")
    num = int(input("请输入要下载的页数："))
    base_url = "https://tieba.baidu.com/f?&ie=utf-8&{}"
    for pn in range(num):
        args = {
            "kw": content,
            "pn": pn*50
        }
        args = urlencode(args)
        file_name = "第" + str(pn+1) + "页.html"
        print("正在保存" + file_name)
        html_byte = get_html(base_url.format(args))
        save_html(file_name, html_byte)

    pass


if __name__ == __name__:
    main()
 ```
## post请求
- 用于用户登陆(无验证码)
- 勾选`preserve log`
```python
from urllib.request import Request, urlopen
from urllib.parse import urlencode
from fake_useragent import UserAgent


url = "???"
form_data = {
    "user": "17853680108"
    "passworld":"12345"
}
headers = {
    "User-Agent": UserAgent().chrome
}
f_data = urlencode(form_data)
request = Request(url, data=f_data.encode(), headers=headers)
response = urlopen(request)
```
## ajax请求的抓取
- 根据滚动条动态显示的网页
- 在检查--XHR里可以显示---Reasponse
- (`www.json.cn`网站可以查看json的程序)
```python
from urllib.request import Request, urlopen
from urllib.parse import urlencode
from fake_useragent import UserAgent

base_url = "https://movie.douban.com/j/chart/top_list?type=17&interval_id=100%3A90&action=&start={}&limit=20"
headers = {
    "User-Agent":UserAgent().chrome
}
while True:
    url = base_url.format(i*20)
    request = Request(url, headers=headers)
    response = urlopen(request)
    info = response.read().decode()
    print(info)
    if info == "" or info is None:
        break
```
## https的请求
- 有ssl证书加密的网站
```python
from urllib.request import Request, urlopen
from fake_useragent import UserAgent
import ssl


url = "https://www.12306.cn"
headers = {
    "User-Agent":UserAgent().chrome
}
request = Request(url, headers = headers)
# 忽略ssl证书
context = ssl._create_default_https_context()
response = urlopen(request)
print(response.read().decode())
```
## 设置代理proxy的使用  
- "快代理"可购买独立ip
```python
from urllib.request import Request, build_opener
from fake_useragent import UserAgent
from urllib.request import ProxyHandler



url = "http://httpbin.org/get"
headers = {
    "User-Agent":UserAgent().chrome
}
request = Request(url, headers = headers)
handler = ProxyHandler({"http":"182.46.251.27:9999"})
opener = build_opener(handler)
response = opener.open(request)
print(response.read().decode())
```