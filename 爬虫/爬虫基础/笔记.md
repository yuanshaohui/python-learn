# 爬虫入门

## 1. 爬虫介绍

**掌握的技术**

- [ ] request库的掌握
- [x] 伪装头
- [x] ajax页面
- [x] 需要ca证书的页面
- [ ] 设置proxy代理

## 2. request请求

- 使用函数
  - `from urllib.request import urlopen`(构建返回内容)（返回：非解码字符串）
  - `from urllib.request import Request` （构建请求对象）
  - `from random import choice`（从伪装头列表中，随机选择）

- 准备目标网址url

- 构建伪装头池

- 构建request对象（传入1.2步骤）

- 将构建的对象传入,urlopen函数

- 利用urlopen函数中的read方法，返回二进制字符串

- 解码输出

  > 改进代码：可以引入`fake_useragent`库来实现调用不同伪装头
  >
  > ```python
  > from fake_useragent import UserAgent 
  > ua = UserAgent()
  > print(ua.chrome)
  > ```
  >
  > 

  ```python
  from urllib.request import urlopen
  from urllib.request import Request
  from random import choice
  
  def main():
      # 发送请求
      url = "https://www.baidu.com/"
  
      # 伪装访问身份
      # 构建伪装头代码池子
      user_agent = ["Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36","Mozilla/5.0(Macintosh;IntelMacOSX10_7_0)AppleWebKit/535.11(KHTML,likeGecko)Chrome/17.0.963.56Safari/535.11","Mozilla/5.0(WindowsNT6.1;rv:2.0.1)Gecko/20100101Firefox/4.0.1"]
      headers = {
          "User-Agent": choice(user_agent)
      }
  
      # 构造请求request
      request = Request(url, headers=headers)
  
      # 构造response对象
      response = urlopen(request)
  
      # 读取内容
      info = response.read()
  
      # 打印内容
      print(info.decode())
  
  
  if __name__ == "__main__":
      main()
  ```

  

## 3. get请求

- 简介：主要用于对不同网址的url访问

- 解决网址中有中文，需要转码的问题
  - `from urllib.parse import quote`方法转码汉语
  - `from urllib.parse import urlencode`传入字典，拼出url

```python
from urllib.request import urlopen
from urllib.request import Request
from random import choice
from urllib.parse import urlencode


def main():
    # 发送请求
    # 传入字典，构建url
    name = input("请输入要搜索的关键字：")
    args = {
        "q":name,
        "ie":"utf-8"
    } 

    url = "https://cn.bing.com/search?{}".format(urlencode(args))

    # 伪装访问身份
    # 构建伪装头代码池子
    user_agent = ["Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36","Mozilla/5.0(Macintosh;IntelMacOSX10_7_0)AppleWebKit/535.11(KHTML,likeGecko)Chrome/17.0.963.56Safari/535.11","Mozilla/5.0(WindowsNT6.1;rv:2.0.1)Gecko/20100101Firefox/4.0.1"]
    headers = {
        "User-Agent": choice(user_agent)
    }

    # 构造请求request
    request = Request(url, headers=headers)

    # 构造response对象
    response = urlopen(request)

    # 读取内容
    info = response.read()

    # 打印内容
    print(info.decode())


if __name__ == "__main__":
    main()
```



## 4. post 请求

**概念**：用于用户登陆的请求

**浏览器检查设置**：network----preservelog(勾选)----找login.html文件----（看请求头里是否有Form Data账户信息））

## 5. ajax请求

**概念**：动态填充页面的请求，例如：[豆瓣排行](https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&type=24&interval_id=100:90&action=)

**浏览器检查设置**：network----XHR(勾选)----在response里----[将内容复制到该网站整理](https://www.json.cn/)

**演示**：爬取豆瓣ajax类型网页

```python
from urllib.request import Request, urlopen
from fake_useragent import UserAgent


def html_one(num):
    # 构建url
    url = "https://movie.douban.com/j/chart/top_list?type=24&interval_id=100%3A90&action=&start={}&limit=1".format(num)

    # 构建伪装头
    ua = UserAgent()
    headers = {
        "User-Agent" : ua.chrome
    }

    # 构建Request
    request = Request(url, headers=headers)

    # 构建返回
    response = urlopen(request)

    # 读取返回值
    recv_data = response.read().decode()
    
    return recv_data



def main():

    # 循环页数
    num = 0
    while True:
        data = html_one(num)
        
        # 判断退出
        if not data or data == "":
            break
        print(data)
        print("---------第{}页-------".format(num/20))
        num += 20

    print("爬取结束")


if __name__ == "__main__":
    main()
```



## 6. https的使用

**概念**：加密的网站，ca证书

**忽略证书**：

- `import ssl`（导入的函数）
- `context = ssl._create_unverified_context()`(写入代码)
  - `response = urlopen(request, context = context`(导入urlopen方法中)



## 7. 设置代理proxy

**概念**：使用代理，来访问网站。

**演示**：

```python
from urllib.request import ProxyHandler
from urllib.request import build_opener

proxy = ProxyHandler({"http":"60.190.23.50"})
opener = build_opener(proxy)
url = "https://cn.bing.com/"
response = opener.open(url)
print(response.read().decode("utf-8"))
```

















## 感谢

B站视频[https://www.bilibili.com/video/av87613645?p=4](https://www.bilibili.com/video/av87613645?p=4)